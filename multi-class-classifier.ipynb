{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error Tunnel connection\n",
      "[nltk_data]     failed: 407 Proxy Authentication Required>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sqlite3 import Error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sqlite3\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"train.csv\",sep=';')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           4000 non-null   int64 \n",
      " 1   summary      4000 non-null   object\n",
      " 2   description  3984 non-null   object\n",
      " 3   label_name   4000 non-null   object\n",
      " 4   label        4000 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing Function\n",
    "def text_processing(dataset):\n",
    "     # Removing contents of tags and all for further text processing\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'<.+?>', value=r' ')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'<.+?>', value=r' ')\n",
    "\n",
    "  # Removing links from all for further text processing\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', value=r' ')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', value=r' ')\n",
    "\n",
    "  # Replace email addresses\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'^.+@[^\\.].*\\.[a-z]{2,}$', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'^.+@[^\\.].*\\.[a-z]{2,}$', value=r'')\n",
    "\n",
    "  # Replace URLs with 'web-address'\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$', value=r'')\n",
    "\n",
    "  # Replace 10 digit phone numbers\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', value=r'')\n",
    "\n",
    "  # Replace numbers with 'numbr'\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'\\d+(\\.\\d+)?', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'\\d+(\\.\\d+)?', value=r'')\n",
    "\n",
    "  # Remove punctuation\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'[^\\w\\d\\s]', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'[^\\w\\d\\s]', value=r'')\n",
    "\n",
    "  #converting to lower case\n",
    "    dataset['description'] = dataset['description'].str.lower()\n",
    "    dataset['summary'] = dataset['summary'].str.lower()\n",
    "\n",
    "  # Removing rows with empty columns \n",
    "    #dataset.dropna(subset=['description','summary'],inplace=True)\n",
    "    dataset.reset_index(inplace = True)\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest=[\"id\",\"summary\",\"description\",\"label_name\",\"label\"]\n",
    "dataset=data[cols_of_interest]\n",
    "dataset_test = text_processing(dataset)\n",
    "dataset[\"Merged\"] = dataset[\"summary\"].str.cat(dataset[\"description\"], sep =\" \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\msg5kor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.set_proxy('https://msg5kor:omsrisai1908@rb-proxy-de.bosch.com:8080')\n",
    "stemmer = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = stopwords.words(\"english\")\n",
    "dataset['cleaned'] = dataset['Merged'].apply(lambda x: \" \".join([lemmatizer.lemmatize(i) for i in re.sub(\"[^a-zA-Z]\", \" \",str(x)).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   index        4000 non-null   int64 \n",
      " 1   id           4000 non-null   int64 \n",
      " 2   summary      4000 non-null   object\n",
      " 3   description  3984 non-null   object\n",
      " 4   label_name   4000 non-null   object\n",
      " 5   label        4000 non-null   int64 \n",
      " 6   Merged       3984 non-null   object\n",
      " 7   cleaned      4000 non-null   object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 250.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 20565)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "final_features = vectorizer.fit_transform(dataset['cleaned']).toarray()\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.33      0.47        12\n",
      "           1       0.89      1.00      0.94         8\n",
      "           2       0.96      0.86      0.91        50\n",
      "           3       0.50      0.55      0.52        11\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       0.19      0.11      0.14        27\n",
      "           6       0.25      0.66      0.36       124\n",
      "           7       0.57      0.67      0.62        18\n",
      "           8       1.00      1.00      1.00        28\n",
      "           9       0.35      0.18      0.24        33\n",
      "          10       0.31      0.24      0.27        45\n",
      "          11       0.88      0.64      0.74        22\n",
      "          12       0.52      0.50      0.51        50\n",
      "          13       0.35      0.39      0.37        18\n",
      "          14       0.40      0.12      0.19        16\n",
      "          15       0.93      0.73      0.82        37\n",
      "          16       0.64      0.54      0.58        13\n",
      "          17       0.88      0.64      0.74        11\n",
      "          18       0.50      0.53      0.51        57\n",
      "          19       0.33      0.15      0.21        13\n",
      "          20       0.36      0.43      0.39        23\n",
      "          21       0.33      0.23      0.27        13\n",
      "          22       0.33      0.25      0.29        12\n",
      "          23       0.42      0.58      0.49        19\n",
      "          24       0.60      0.07      0.12        46\n",
      "          25       0.00      0.00      0.00        13\n",
      "          26       0.77      0.81      0.79        21\n",
      "          27       0.25      0.08      0.12        12\n",
      "          28       0.52      0.65      0.58        26\n",
      "          29       1.00      0.08      0.14        13\n",
      "          30       0.00      0.00      0.00        15\n",
      "          31       0.50      0.14      0.22         7\n",
      "          32       0.25      0.29      0.27         7\n",
      "          33       0.69      0.50      0.58        22\n",
      "          34       0.00      0.00      0.00         8\n",
      "          35       0.50      0.20      0.29         5\n",
      "          36       1.00      0.18      0.30        17\n",
      "          37       0.47      0.67      0.55        12\n",
      "          38       0.20      0.07      0.10        15\n",
      "          39       0.22      0.50      0.31         4\n",
      "          40       1.00      0.14      0.25         7\n",
      "          41       0.54      0.57      0.55        23\n",
      "          42       0.75      0.19      0.30        16\n",
      "          43       0.45      0.56      0.50         9\n",
      "          44       0.69      0.92      0.79        12\n",
      "          45       0.00      0.00      0.00         4\n",
      "          46       0.33      0.17      0.22         6\n",
      "          47       0.18      0.20      0.19        10\n",
      "          48       0.67      0.40      0.50         5\n",
      "\n",
      "    accuracy                           0.46      1000\n",
      "   macro avg       0.52      0.40      0.41      1000\n",
      "weighted avg       0.52      0.46      0.44      1000\n",
      "\n",
      "[[ 4  0  1 ...  0  0  0]\n",
      " [ 0  8  0 ...  0  0  0]\n",
      " [ 0  0 43 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  1  3  0]\n",
      " [ 0  0  0 ...  1  2  0]\n",
      " [ 0  0  0 ...  0  0  2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# this block is to split the dataset into training and testing set \n",
    "X = dataset['cleaned']\n",
    "Y = dataset['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "# instead of doing these steps one at a time, we can use a pipeline to complete then all at once\n",
    "pipeline = Pipeline([('vect', vectorizer),\n",
    "                     ('chi',  SelectKBest(chi2, k=1200)),\n",
    "                     ('clf', LinearSVC())])\n",
    "\n",
    "\n",
    "\n",
    "# fitting our model and save it in a pickle for later use\n",
    "model = pipeline.fit(X_train, y_train)\n",
    "with open('RandomForest.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "ytest = np.array(y_test)\n",
    "\n",
    "# confusion matrix and classification report(precision, recall, F1-score)\n",
    "print(classification_report(ytest, model.predict(X_test)))\n",
    "print(confusion_matrix(ytest, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_test = pd.read_csv(\"test.csv\",sep=';')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing Function\n",
    "def text_processing(dataset):\n",
    "     # Removing contents of tags and all for further text processing\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'<.+?>', value=r' ')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'<.+?>', value=r' ')\n",
    "\n",
    "  # Removing links from all for further text processing\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', value=r' ')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', value=r' ')\n",
    "\n",
    "  # Replace email addresses\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'^.+@[^\\.].*\\.[a-z]{2,}$', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'^.+@[^\\.].*\\.[a-z]{2,}$', value=r'')\n",
    "\n",
    "  # Replace URLs with 'web-address'\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$', value=r'')\n",
    "\n",
    "  # Replace 10 digit phone numbers\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', value=r'')\n",
    "\n",
    "  # Replace numbers with 'numbr'\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'\\d+(\\.\\d+)?', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'\\d+(\\.\\d+)?', value=r'')\n",
    "\n",
    "  # Remove punctuation\n",
    "    dataset['description'].replace(regex=True,inplace=True, to_replace= r'[^\\w\\d\\s]', value=r'')\n",
    "    dataset['summary'].replace(regex=True,inplace=True, to_replace= r'[^\\w\\d\\s]', value=r'')\n",
    "\n",
    "  #converting to lower case\n",
    "    dataset['description'] = dataset['description'].str.lower()\n",
    "    dataset['summary'] = dataset['summary'].str.lower()\n",
    "\n",
    "  # Removing rows with empty columns \n",
    "    #dataset.dropna(subset=['description','summary'],inplace=True)\n",
    "    dataset.reset_index(inplace = True)\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest=[\"id\",\"summary\",\"description\"]\n",
    "dataset_test=data_test[cols_of_interest]\n",
    "dataset_test = text_processing(dataset_test)\n",
    "dataset_test[\"Merged\"] = dataset_test[\"summary\"].str.cat(dataset_test[\"description\"], sep =\" \\n \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test[\"Merged\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.set_proxy('https://msg5kor:omsrisai1908@rb-proxy-de.bosch.com:8080')\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "dataset_test['cleaned'] = dataset_test['Merged'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \",str(x)).split() if i not in words]).lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "final_features = vectorizer.fit_transform(dataset_test['cleaned']).toarray()\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = dataset_test['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = np.transpose(predictions)\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pred in enumerate(predictions):\n",
    "    if pred < 0:\n",
    "        predictions[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['pred'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('my_submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
